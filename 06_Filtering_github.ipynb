{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinacaramaschi/TPT-PE-thematic-analysis/blob/main/06_Filtering_github.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T923j7MxsEMg"
      },
      "source": [
        "# Data Filtering for TPT/PE Articles\n",
        "\n",
        "This script takes the scraped text, which has been filtered for specific articles that we need removed (like announcements) and overlapping text, and does additional filtering in preparation for lemmatization and bi-grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "8Br8BKkrsEMj",
        "outputId": "64328189-51c1-4a34-8524-9f88cadf92e3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.container { width:95% !important; }</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Print out  all expressions\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\" #default 'last_expr'\n",
        "# Wider cells\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CotOoGoNsmJY",
        "outputId": "ed65de3a-f640-40dc-fa70-a823c2e7e743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvLmx1gssEMl"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Import regular expressions, for data processing\n",
        "import re\n",
        "\n",
        "import gensim\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from gensim.corpora import Dictionary\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag\n",
        "nltk.download('wordnet',quiet=True)\n",
        "#nltk.download('punkt',quiet=True)   #required by word_tokenize method\n",
        "nltk.download('averaged_perceptron_tagger',quiet=True) #required by pos_tag method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ4KgOa1sEMm"
      },
      "source": [
        "## Reading the datafile\n",
        "\n",
        "First, we import the pickle file that holds all of the scraped PDF data. This data is then put into a Pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6Ha0zfusEMn"
      },
      "outputs": [],
      "source": [
        "directory_name = '/content/drive/MyDrive/Colab Notebooks/TPT_PE_review/'\n",
        "\n",
        "tpt_df = pd.read_pickle(directory_name + 'TPT_metadata_final.pkl')\n",
        "tpt_df=tpt_df.reset_index();"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pe_df = pd.read_pickle(directory_name + 'physics_education_metadata_final.pkl')\n",
        "pe_df=pe_df.reset_index();\n"
      ],
      "metadata": {
        "id": "87jsLwYhs9yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tpt_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQY9-FfbsW8e",
        "outputId": "085b08a0-84b6-4229-ef8f-64fc4f607015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['level_0', 'index', 'filename', 'year', 'title', 'author_list',\n",
              "       'volume', 'issue', 'processed_len', 'page', 'page_len', 'overlap',\n",
              "       'pdf2fix', 'pdf_pages', 'overlapnext', 'overlapprev', 'URL',\n",
              "       'processed', 'raw', 'page_start', 'page_end', 'publisher',\n",
              "       'filename_orig', 'subtitle', 'authors', 'author', 'editor',\n",
              "       'reference-count', 'is-referenced-by-count', 'issued', 'link', 'doi',\n",
              "       'fulltext', 'first_n_words', 'cleaned_fulltext', 'word_count',\n",
              "       'extracted_text', 'flag_before'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "collapsed": true,
        "id": "8P_bOTJJsEMn"
      },
      "outputs": [],
      "source": [
        "tpt_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5-hDxwpsEMn",
        "outputId": "0bede94c-9a18-4741-8c1d-ac730dfd6ab0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['index', 'title', 'authors', 'publication_year', 'doi', 'volume',\n",
              "       'issue', 'fpage', 'lpage', 'pdf_filename', 'zip_filename', 'fulltext',\n",
              "       'word_count', 'extracted_text', 'flag_before'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "pe_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pe_df.head()"
      ],
      "metadata": {
        "id": "doi2YjoRumq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku9q9ZYesEMo"
      },
      "source": [
        "### Start processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2F6wKCuQsEMo"
      },
      "outputs": [],
      "source": [
        "field='extracted_text'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ml1XuOBsEMp"
      },
      "outputs": [],
      "source": [
        "def ML_process(text):\n",
        "    filt_text = text\n",
        "    #remove 'cid'\n",
        "    filt_text = re.sub('\\W(cid:\\d{0,3})\\W', '', filt_text) #Symbols such as @\n",
        "    #remove some words in all-caps\n",
        "    #USELESS NOW cause RawTextProcesser_* converted everything to lower case\n",
        "    filt_text = re.sub(r'(?<=\\W)(INTRODUCTION|CONCLUSION[S]?|BACKGROUND|ABSTRACT|ANALYSIS|EXPERIMENTAL|METHOD[S]?|METHODOLOGY|MOTIVATION[S]?|PRELIMINARY|RESULTS|APPLICATIONS|CONCLUDING|IMPLEMENTATION|EVALUATION|REMARKS|DISCUSSION[S]?|ACKNOWLEDGEMENTS|FUTURE PLANS|FUTURE WORK|FUTURE REASEARCH|SUMMARY|FIGURE[S]?|FIG|TABLE|I\\.|II|III|IV|VI{0,3}|IX|X|XI{0,3})(?=\\W)',\n",
        "                       '', filt_text)\n",
        "    #remove newlines, tabs, etc. also remove digits (\\d) and bullet points (\\uf0b7)\n",
        "    filt_text = re.sub('[\\t\\n\\r\\f\\v\\d\\uf0b7]', ' ', filt_text)\n",
        "    #removes all special characters that aren't numbers or letters\n",
        "    filt_text = re.sub('[^A-Za-z0-9]+', ' ', filt_text)\n",
        "    #split lines\n",
        "    filt_text = re.sub('- ', '', filt_text)\n",
        "    #to lower case\n",
        "    filt_text = filt_text.lower()\n",
        "\n",
        "    #tlie -> the\n",
        "    filt_text = re.sub(' tlie ', ' the ', filt_text)\n",
        "    #per cent -> percent\n",
        "    filt_text = re.sub(' per cent ', ' percent ', filt_text)\n",
        "    # )ed -> fied\n",
        "    #filt_text = re.sub(re.escape(' \\)ed '), 'fied ', filt_text)\n",
        "    # - cation -> cation\n",
        "    #filt_text = re.sub('- cation ', 'cation ', filt_text)\n",
        "    return filt_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYq5Y4h-sEMp"
      },
      "source": [
        "Now, we apply text processing to the entire dataset, creating a processed version.  We then print an example of the processed version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqTUe7-OsEMp"
      },
      "outputs": [],
      "source": [
        "tpt_df['extracted_text_processedby06'] = tpt_df[field].map(lambda x: ML_process(x))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pe_df['extracted_text_processedby06'] = pe_df[field].map(lambda x: ML_process(x))"
      ],
      "metadata": {
        "id": "va7CRagRwJwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the dataframes as pickle files\n",
        "tpt_df.to_pickle(directory_name + '06_filtered_TPT_V1.pkl')\n",
        "pe_df.to_pickle(directory_name + '06_filtered_PE_V1.pkl')"
      ],
      "metadata": {
        "id": "GGwNHI2cwwyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjhcYzJUsEMq"
      },
      "source": [
        "### Continue to 07 Tokenize_MakeBigrams.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ubQLcjfbEsZ5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}