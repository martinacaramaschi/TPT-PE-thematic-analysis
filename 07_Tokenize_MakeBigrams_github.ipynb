{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinacaramaschi/TPT-PE-thematic-analysis/blob/main/07_Tokenize_MakeBigrams_github.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        },
        "id": "iD56A60ax81R",
        "outputId": "b53829f3-d513-4934-bed3-bfe2c2c4725d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.container { width:95% !important; }</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Print out  all expressions\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\" #default 'last_expr'\n",
        "# Wider cells\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cu9zcoHZ0Tp0",
        "outputId": "4d6e054f-6227-4b27-d739-7ea56497e116"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-glVleL2x81U",
        "outputId": "57ffeaeb-8883-4f1c-ab5a-5a0614a290d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Imports\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag\n",
        "nltk.download('wordnet',quiet=True)\n",
        "nltk.download('punkt',quiet=True)   #required by word_tokenize method\n",
        "nltk.download('averaged_perceptron_tagger',quiet=True) #required by pos_tag method\n",
        "\n",
        "#Import regular expressions, for data processing\n",
        "import re\n",
        "\n",
        "import gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THul87wgx81V"
      },
      "outputs": [],
      "source": [
        "directory_name = '/content/drive/MyDrive/Colab Notebooks/TPT_PE_review/'\n",
        "\n",
        "tpt_df = pd.read_pickle(directory_name + '06_filtered_TPT_V1.pkl')\n",
        "pe_df = pd.read_pickle(directory_name + '06_filtered_PE_V1.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tpt_df.shape)\n",
        "pe_df.shape"
      ],
      "metadata": {
        "id": "mvzZS4OjSWiC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9cffdf3-c65d-4ece-96c7-6574e4c62365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7203, 39)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6445, 16)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tpt_df.columns\n",
        "pe_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deut4m4sT0Ds",
        "outputId": "bf718c2c-015a-452a-c759-a439770c19d6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['level_0', 'index', 'filename', 'year', 'title', 'author_list',\n",
              "       'volume', 'issue', 'processed_len', 'page', 'page_len', 'overlap',\n",
              "       'pdf2fix', 'pdf_pages', 'overlapnext', 'overlapprev', 'URL',\n",
              "       'processed', 'raw', 'page_start', 'page_end', 'publisher',\n",
              "       'filename_orig', 'subtitle', 'authors', 'author', 'editor',\n",
              "       'reference-count', 'is-referenced-by-count', 'issued', 'link', 'doi',\n",
              "       'fulltext', 'first_n_words', 'cleaned_fulltext', 'word_count',\n",
              "       'extracted_text', 'flag_before', 'extracted_text_processedby06'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['index', 'title', 'authors', 'publication_year', 'doi', 'volume',\n",
              "       'issue', 'fpage', 'lpage', 'pdf_filename', 'zip_filename', 'fulltext',\n",
              "       'word_count', 'extracted_text', 'flag_before',\n",
              "       'extracted_text_processedby06'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqFdZuIz28XW"
      },
      "source": [
        "## Creating the complete dataset TPT+PE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E25UWaWx6s15"
      },
      "outputs": [],
      "source": [
        "# cretes a new df containing all tpt_df and pe_df, only including specific columns\n",
        "new_tpt_df = tpt_df[['index', 'title', 'authors', 'year', 'doi', 'volume', 'issue',\n",
        "                     'filename', 'fulltext', 'extracted_text', 'extracted_text_processedby06'\n",
        "                     ]].rename(columns={'filename': 'pdf_filename'}).copy()\n",
        "new_pe_df = pe_df[['index', 'title', 'authors', 'publication_year', 'doi', 'volume',\n",
        "                   'issue', 'pdf_filename', 'fulltext', 'extracted_text', 'extracted_text_processedby06'\n",
        "                   ]].rename(columns={'publication_year': 'year'}).copy()\n",
        "\n",
        "new_tpt_df['journal'] = 'TPT'\n",
        "new_pe_df['journal'] = 'PE'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_tpt_df.head()"
      ],
      "metadata": {
        "id": "veJlKH97UPuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_pe_df.head()"
      ],
      "metadata": {
        "id": "mkPfMqSQUT_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the combined dataset"
      ],
      "metadata": {
        "id": "amOqDAPzG18g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# combine the two df\n",
        "combined_df = pd.concat([new_tpt_df, new_pe_df], ignore_index=True)\n",
        "\n",
        "# shuffle the elements of combined_df and call them combined_pe_tpt_df\n",
        "combined_pe_tpt_df = combined_df.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "Aw8xOi9CUMcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_pe_tpt_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qzkx5Z1lUwD7",
        "outputId": "ebfa5ba2-7ad0-49a0-811a-f7d1cae05b5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13648, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Saving combined dataset**"
      ],
      "metadata": {
        "id": "xHhLplisMgdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_pe_tpt_df.to_pickle(directory_name + '06_filtered_combined_V2.pkl')"
      ],
      "metadata": {
        "id": "hNyJAK90VxLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_pe_tpt_df.head()"
      ],
      "metadata": {
        "id": "lhV5JJ5p8c4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNfi40KNSAH3"
      },
      "source": [
        "### **Datasets names**\n",
        "\n",
        "\n",
        "*   TPT --> new_tpt_df\n",
        "*   PE --> new_pe_df\n",
        "*   TPT + PE --> combined_pe_tpt_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5PYF2ixx81V"
      },
      "source": [
        "## **Removing stopwords and stemming**\n",
        "\n",
        "Now, we can remove the stopwords and do the stemming, leaving us with a list of documents, each of which is essentially a tokenized list of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDP5mjxBx81W",
        "outputId": "47dbf7d3-faf2-4256-8bc7-2be395fb7d52",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TPT:\n",
            " Index(['index', 'title', 'authors', 'year', 'doi', 'volume', 'issue',\n",
            "       'pdf_filename', 'fulltext', 'extracted_text',\n",
            "       'extracted_text_processedby06', 'journal'],\n",
            "      dtype='object')\n",
            "PE:\n",
            " Index(['index', 'title', 'authors', 'year', 'doi', 'volume', 'issue',\n",
            "       'pdf_filename', 'fulltext', 'extracted_text',\n",
            "       'extracted_text_processedby06', 'journal'],\n",
            "      dtype='object')\n",
            "TPT + PE:\n",
            " Index(['index', 'title', 'authors', 'year', 'doi', 'volume', 'issue',\n",
            "       'pdf_filename', 'fulltext', 'extracted_text',\n",
            "       'extracted_text_processedby06', 'journal'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(\"TPT:\\n\", new_tpt_df.columns)\n",
        "print(\"PE:\\n\", new_pe_df.columns)\n",
        "print(\"TPT + PE:\\n\",combined_pe_tpt_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRNCJLXAx81X"
      },
      "outputs": [],
      "source": [
        "field='extracted_text_processedby06'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9RrRo-Gx81X"
      },
      "outputs": [],
      "source": [
        "def sent_to_words(list_sentences):\n",
        "    return [gensim.utils.simple_preprocess(str(sentence), deacc=True) for sentence in list_sentences]  #deacc=True removes accent marks from tokens (incl. punctuations)\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    return [[word for word in gensim.utils.simple_preprocess(str(doc)) if word not in STOPWORDS ] for doc in tokens]\n",
        "\n",
        "def get_wordnet_pos(word): #Provide a POS tag\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN) #return NOUN by default\n",
        "\n",
        "def lemmatize_token(token):\n",
        "    return nltk.stem.WordNetLemmatizer().lemmatize(token, get_wordnet_pos(token))\n",
        "\n",
        "def lemmatize(token_list):\n",
        "    '''Input example: [\"he\", \"matches\", \"the\", \"profile\"]'''\n",
        "    return [lemmatize_token(token) for token in token_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSbRwiZRUmu3"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from gensim.corpora import Dictionary\n",
        "!pip install nltk\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag\n",
        "nltk.download('wordnet',quiet=True)\n",
        "#nltk.download('punkt',quiet=True)   #required by word_tokenize method\n",
        "nltk.download('averaged_perceptron_tagger',quiet=True) #required by pos_tag method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6UEMu1hx81X"
      },
      "outputs": [],
      "source": [
        "#Tokenize documents\n",
        "#data_words_tpt = sent_to_words(new_tpt_df[field])\n",
        "#data_words_pe = sent_to_words(new_pe_df[field])\n",
        "data_words_combined = sent_to_words(combined_pe_tpt_df[field])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmLPf_4Ax81Y",
        "outputId": "9395d22d-bc06-4c21-f74e-0f942b029d08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 58.8 s, sys: 879 ms, total: 59.6 s\n",
            "Wall time: 1min 1s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "#Remove stopwords\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "#data_words_tpt_nostops = remove_stopwords(data_words_tpt)\n",
        "#data_words_pe_nostops = remove_stopwords(data_words_pe)\n",
        "data_words_combined_nostops = remove_stopwords(data_words_combined)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJFHgfkHWxQx",
        "outputId": "05dab6d9-5fe6-4a4d-9ace-377ff2117ed0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng',quiet=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Lemmatizing**"
      ],
      "metadata": {
        "id": "VEL3yJxxL4wN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fGDPW5Ax81Y",
        "outputId": "fb22b094-ebc8-41c3-ff45-fe4a32637bdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 11min 2s, sys: 2.53 s, total: 11min 4s\n",
            "Wall time: 11min 28s\n"
          ]
        }
      ],
      "source": [
        "#TPT\n",
        "%%time\n",
        "data_words_tpt_lemmatized = [lemmatize(token_list) for token_list in data_words_tpt_nostops]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TaRp2iHba_pA",
        "outputId": "0eabfb15-60a9-4650-d75a-95acb132b26b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 11min 50s, sys: 2.25 s, total: 11min 52s\n",
            "Wall time: 12min 1s\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#PE\n",
        "%%time\n",
        "data_words_pe_lemmatized = [lemmatize(token_list) for token_list in data_words_pe_nostops]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jX23off_fhYy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59ff2384-bab9-4209-ccef-789e488522f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 22min 44s, sys: 4.17 s, total: 22min 48s\n",
            "Wall time: 23min 9s\n"
          ]
        }
      ],
      "source": [
        "#combined\n",
        "%%time\n",
        "data_words_combined_lemmatized = [lemmatize(token_list) for token_list in data_words_combined_nostops]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking dimension of the combined dataset"
      ],
      "metadata": {
        "id": "1t8GAZb3MM1l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ConN3Sdcx81Y",
        "outputId": "bd414f25-3ba8-4d2e-9703-98bef5ac2ce2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13648"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "#len(data_words_tpt_lemmatized)\n",
        "#len(data_words_pe_lemmatized)\n",
        "len(data_words_combined_lemmatized)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Making bigrams**"
      ],
      "metadata": {
        "id": "LZUEbJe_LoSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TPT\n",
        "phrases = gensim.models.phrases.Phrases(data_words_tpt_lemmatized, min_count=10)\n",
        "bigram = gensim.models.phrases.Phraser(phrases)\n",
        "#bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "def make_bigrams(list_tokenized_docs):\n",
        "    '''Input example: [['He','matches','the','profile']]'''\n",
        "    return [bigram[doc] for doc in list_tokenized_docs]\n",
        "\n",
        "# Form Bigrams\n",
        "data_words_bigrams_tpt = make_bigrams(data_words_tpt_lemmatized)"
      ],
      "metadata": {
        "id": "I_m325jeomYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PE\n",
        "phrases = gensim.models.phrases.Phrases(data_words_pe_lemmatized, min_count=10)\n",
        "bigram = gensim.models.phrases.Phraser(phrases)\n",
        "#bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "def make_bigrams(list_tokenized_docs):\n",
        "    '''Input example: [['He','matches','the','profile']]'''\n",
        "    return [bigram[doc] for doc in list_tokenized_docs]\n",
        "\n",
        "data_words_bigrams_pe = make_bigrams(data_words_pe_lemmatized)"
      ],
      "metadata": {
        "id": "Ir8QR8aNozVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#combined\n",
        "phrases = gensim.models.phrases.Phrases(data_words_combined_lemmatized, min_count=10)\n",
        "bigram = gensim.models.phrases.Phraser(phrases)\n",
        "#bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "def make_bigrams(list_tokenized_docs):\n",
        "    '''Input example: [['He','matches','the','profile']]'''\n",
        "    return [bigram[doc] for doc in list_tokenized_docs]\n",
        "\n",
        "data_words_bigrams_combined = make_bigrams(data_words_combined_lemmatized)"
      ],
      "metadata": {
        "id": "6YhmGqzDp1mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Saving bigrams**"
      ],
      "metadata": {
        "id": "eZRr649dLtss"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H56_SrK7x81Z"
      },
      "outputs": [],
      "source": [
        "#with open(directory_name+'07_bigrams_TPT_V1.pkl','wb') as output: pickle.dump(data_words_bigrams_tpt,output)\n",
        "#with open(directory_name+'07_bigrams_PE_V1.pkl','wb') as output: pickle.dump(data_words_bigrams_pe,output)\n",
        "with open(directory_name+'07_bigrams_combined_V2.pkl','wb') as output: pickle.dump(data_words_bigrams_combined,output)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}